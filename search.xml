<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>kubectl命令表</title>
    <url>/2022/05/28/kubectl%E5%91%BD%E4%BB%A4%E8%A1%A8/</url>
    <content><![CDATA[<h2 id="kubectl-label"><a href="#kubectl-label" class="headerlink" title="kubectl label"></a><code>kubectl label</code></h2><span id="more"></span>

<p>更新（增加、修改或删除）资源上的 label（标签）。</p>
<ul>
<li>label 必须以字母或数字开头，可以使用字母、数字、连字符、点和下划线，最长63个字符。</li>
<li>如果–overwrite 为 true，则可以覆盖已有的 label，否则尝试覆盖 label 将会报错。</li>
<li>如果指定了–resource-version，则更新将使用此资源版本，否则将使用现有的资源版本。</li>
</ul>
<p>语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]</span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>给名为foo的Pod添加label unhealthy&#x3D;true。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label pods foo unhealthy=true</span><br></pre></td></tr></table></figure>

<p>给名为foo的Pod修改label 为 ‘status’ &#x2F; value ‘unhealthy’，且覆盖现有的value。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label --overwrite pods foo status=unhealthy</span><br></pre></td></tr></table></figure>

<p>给 namespace 中的所有 pod 添加 label</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label pods --all status=unhealthy</span><br></pre></td></tr></table></figure>

<p>仅当resource-version&#x3D;1时才更新 名为foo的Pod上的label。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label pods foo status=unhealthy --resource-version=1</span><br></pre></td></tr></table></figure>

<p>删除名为“bar”的label 。（使用“ - ”减号相连）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label pods foo bar-</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>kubernetes操作</title>
    <url>/2022/06/07/kubernetes%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<span id="more"></span>

<h3 id="授予用户权限"><a href="#授予用户权限" class="headerlink" title="授予用户权限"></a>授予用户权限</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile</span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>

<h3 id="登录pod里面容器"><a href="#登录pod里面容器" class="headerlink" title="登录pod里面容器"></a>登录pod里面容器</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl exec $pod -n $namespace -c $container -i -t -- bash：进入指定pod的指定容器（查看pod内有哪些容器的命令：kubectl get pods $pod -n $namespace -o jsonpath=&#123;.spec.containers[*].name&#125;）</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>kubernetes操作</title>
    <url>/2022/06/08/kubernetes%E6%93%8D%E4%BD%9C-1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/</url>
    <content><![CDATA[<h2 id="调度器介绍"><a href="#调度器介绍" class="headerlink" title="调度器介绍"></a>调度器介绍</h2><blockquote>
<p>调度器通过 <code>kubernetes</code> 的 <code>watch</code> 机制来发现集群中新创建且尚未被调度到 <code>Node</code> 上的 <code>Pod</code>。调度器会将发现的每一个未调度的 <code>Pod</code> 调度到一个合适的 <code>Node</code> 上来运行。</p>
<p><code>kube-scheduler</code> 是 <code>Kubernetes</code> 集群的默认调度器，并且是集群控制面的一部分。</p>
<p>在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件&#x2F;软件&#x2F;策略限 制、亲和以及反亲和要求、数据局域性、负载间的干扰等等。</p>
</blockquote>
<span id="more"></span>

<h3 id="节点标签"><a href="#节点标签" class="headerlink" title="节点标签"></a>节点标签</h3><p>查看现有node及label</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get node --show-labels</span><br></pre></td></tr></table></figure>

<p>添加label</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;</span><br></pre></td></tr></table></figure>

<p>删除label</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;-</span><br></pre></td></tr></table></figure>



<h3 id="nodeName"><a href="#nodeName" class="headerlink" title="nodeName"></a><code>nodeName</code></h3><p><code>nodeName</code> 是节点选择约束的最简单方法，但一般不推荐。如果 <code>nodeName</code> 在 <code>PodSpec</code> 中指定了，则它优先于其他的节点选择方法。</p>
<p>使用 <code>nodeName</code> 来选择节点的一些限制：<br>• 如果指定的节点不存在。<br>• 如果指定的节点没有资源来容纳 pod，则pod 调度失败。<br>• 云环境中的节点名称并非总是可预测或稳定的。<br><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525172114834.png" alt="image-20220525172114834"></p>
<p>指定的节点没有资源来容纳pod，则pod调度失败</p>
<h3 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a><code>nodeSelector</code></h3><h4 id="通过标签选择节点"><a href="#通过标签选择节点" class="headerlink" title="通过标签选择节点"></a>通过标签选择节点</h4><p><code>nodeSelector</code> 是节点选择约束的最简单推荐形式。<br>给选择的节点添加标签： </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label nodes server2 disktype=ssd</span><br></pre></td></tr></table></figure>

<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525172541122.png" alt="image-20220525172541122"></p>
<p>如果没有标签会无法运行（pending）</p>
<h4 id="亲和与反亲和"><a href="#亲和与反亲和" class="headerlink" title="亲和与反亲和"></a>亲和与反亲和</h4><p><code>nodeSelector</code> 提供了一种非常简单的方法来将 pod 约束到具有特定标签的节点上。亲和&#x2F;反亲和功能极大地扩展了你可以表达约束的类型。 可以发现规则是“软”&#x2F;“偏好”，而不是硬性要求，因此，如果调度器无 法满足该要求，仍然调度该 <code>pod</code><br>你可以使用节点上的 <code>pod</code> 的标签来约束，而不是使用节点本身的标签，来允许哪些 <code>pod</code> 可以或者不可以被放置在一起。</p>
<h5 id="1、节点亲和"><a href="#1、节点亲和" class="headerlink" title="1、节点亲和"></a>1、节点亲和</h5><p>• <code>requiredDuringSchedulingIgnoredDuringExecution</code> 必须满足<br>• <code>preferredDuringSchedulingIgnoredDuringExecution</code> 倾向满足<br>• <code>IgnoreDuringExecution</code> 表示如果在Pod运行期间Node的标签发生变化，导致 亲和性策略不能满足，则继续运行当前的Pod。</p>
<p><code>nodeaffinity</code>还支持多种规则匹配条件的配置：<br>• In：label 的值在列表内<br>• NotIn：label 的值不在列表内<br>• Gt：label 的值大于设置的值，不支持Pod亲和性<br>• Lt：label 的值小于设置的值，不支持pod亲和性<br>• Exists：设置的label 存在<br>• DoesNotExist：设置的 label 不存在</p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173028476.png" alt="image-20220525173028476"></p>
<h5 id="2、pod-亲和性和反亲和性"><a href="#2、pod-亲和性和反亲和性" class="headerlink" title="2、pod 亲和性和反亲和性"></a>2、pod 亲和性和反亲和性</h5><p><code>pod</code> 亲和性和反亲和性<br>• <code>podAffinity</code> 主要解决POD可以和哪些POD部署在同一个拓扑域中的问题 （拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone等。）<br>• <code>podAntiAffinity</code>主要解决POD不能和哪些POD部署在同一个拓扑域中的问题。 它们处理的是Kubernetes集群内部POD和POD之间的关系。<br>• Pod 间亲和与反亲和在与更高级别的集合（例如 ReplicaSets，StatefulSets， Deployments 等）一起使用时，它们可能更加有用。可以轻松配置一组应位于相同定义拓扑（例如，节点）中的工作负载。</p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173214167.png" alt="image-20220525173214167"></p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173237352.png" alt="image-20220525173237352"></p>
<h5 id="3、节点亲和性"><a href="#3、节点亲和性" class="headerlink" title="3、节点亲和性"></a>3、节点亲和性</h5><p>NodeAffinity节点亲和性，是Pod上定义的一种属性，使Pod能够按我们的要求调度到某个Node上，而Taints则恰恰相反，它可以让Node拒绝运行Pod，甚至驱逐Pod。</p>
<p>• Taints(污点)是Node的一个属性，设置了Taints后，所以Kubernetes是不会将Pod调度到这个Node上的，于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要 Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。<br>• 可以使用命令 kubectl taint 给节点增加一个 taint：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes node1 key=value:NoSchedule //创建</span><br><span class="line">kubectl describe nodes server1 |grep Taints //查询</span><br><span class="line">kubectl taint nodes node1 key:NoSchedule- //删除</span><br></pre></td></tr></table></figure>

<p>其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ]<br>• NoSchedule：POD 不会被调度到标记为 taints 节点。<br>• PreferNoSchedule：NoSchedule 的软策略版本。<br>• NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 POD 没有对应 Tolerate 设置，会直接被逐出。</p>
<h6 id="4-1、污点的查询与删除"><a href="#4-1、污点的查询与删除" class="headerlink" title="4.1、污点的查询与删除"></a>4.1、污点的查询与删除</h6><p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173608377.png" alt="image-20220525173608377"></p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173627596.png" alt="image-20220525173627596"></p>
<h6 id="4-2、污点的添加与容忍"><a href="#4-2、污点的添加与容忍" class="headerlink" title="4.2、污点的添加与容忍"></a>4.2、污点的添加与容忍</h6><p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173707203.png" alt="image-20220525173707203"></p>
<p>tolerations中定义的key、value、effect，要与node上设置的taint保持一直：<br>• 如果 operator 是 Exists ，value可以省略。<br>• 如果 operator 是 Equal ，则key与value之间的关系必须相等。<br>• 如果不指定operator属性，则默认值为Equal。</p>
<p>还有两个特殊值：<br>• 当不指定key，再配合Exists 就能匹配所有的key与value ，可以容忍所有污点。<br>• 当不指定effect ，则匹配所有的effect。</p>
<p>在PodSpec中为容器设定容忍标签：</p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525173941245.png" alt="image-20220525173941245"></p>
<p><img src="/2022/05/25/kubernetes%E8%B0%83%E5%BA%A6/image-20220525174011529.png" alt="image-20220525174011529"></p>
]]></content>
  </entry>
  <entry>
    <title>kubernetes资源</title>
    <url>/2022/06/06/kubernetes%E8%B5%84%E6%BA%90/</url>
    <content><![CDATA[<p><a href="https://jimmysong.io/kubernetes-handbook/practice/node-installation.html">https://jimmysong.io/kubernetes-handbook/practice/node-installation.html</a></p>
<p><a href="https://github.com/huweihuang/kubernetes-notes">huweihuang&#x2F;kubernetes-notes: Kubernetes 学习笔记-https://www.huweihuang.com/kubernetes-notes/ (github.com)</a></p>
]]></content>
  </entry>
  <entry>
    <title>KubeVirt</title>
    <url>/2022/10/14/KubeVirt/</url>
    <content><![CDATA[<p>参考文档：<a href="https://zhuanlan.zhihu.com/p/113568026">Kubernetes 管理虚拟机之 KubeVirt - 知乎 (zhihu.com)</a></p>
<p>[<a href="https://developer.aliyun.com/article/888553">没接触过kubevirt？]15分钟快速入门kubevirt-阿里云开发者社区 (aliyun.com)</a></p>
<p><a href="https://remimin.github.io/2018/09/14/kubevirt/">kubevirt以容器方式运行虚拟机 - 敏的博客 | Min’s Blog (remimin.github.io)</a></p>
<span id="more"></span>

<h2 id="容器-amp-虚拟机"><a href="#容器-amp-虚拟机" class="headerlink" title="容器 &amp; 虚拟机"></a>容器 &amp; 虚拟机</h2><p>随着<code>Docker</code>和<code>Kubernetes</code>生态圈的发展，云计算领域对容器的兴趣达到了狂热的程度。 容器技术为应用程序提供了隔离的运行空间，每个容器内都包含一个独享的完整用户环境空间， 容器内的变动不会影响其他容器的运行环境。因为容器之间共享同一个系统内核，当同一个库被多个容器使用时， 内存的使用效率会得到提升。基于物理主机操作系统内核的，那就意味着对于不同内核或者操作系统需求的应用是不可能部署在一起的。</p>
<p>虚拟化技术则是提供了一个完整的虚拟机，为用户提供了不依赖于宿主机内核的运行环境。 对于从物理服务器过渡到虚拟服务器是一个很自然的过程，从用户使用上并没有什么区别。 容器与虚拟机当前看来并不是一个非此即彼的关系，至于采用那种方式去运行应用需要根据具体需求去决定。 在这里笔者并不讨论这个问题，且笔者仅是容器服务的入门玩家，内容若有不准确之处还望斧正。</p>
<p><code>kubernetes</code>提供了较灵活的容器调度和管理能力，那么虚拟机能否像容器一样被<code>k8s</code>管理调度， 充分利用<code>k8s</code>的故障发现，滚动升级等管理机制呢。 在Linux操作系统中虚拟机本质上就是一个操作系统进程应该是可以运行在容器内部的。</p>
<p>目前Redhat开源的<code>kubevirt</code>和Mirantis开源的<code>virtlet</code>都提供了以容器方式运行虚拟机的方案， 至于两者之间的区别，可以看下这篇Mirantis的 <a href="https://www.mirantis.com/blog/kubevirt-vs-virtlet-comparison-better/">blog</a>。</p>
<p>本文将详细介绍<code>kubevirt</code>项目如何实现运行容器化的虚拟机。</p>
<h2 id="什么是kubevirt"><a href="#什么是kubevirt" class="headerlink" title="什么是kubevirt"></a>什么是kubevirt</h2><p>kubevirt是Redhat开源的以容器方式运行虚拟机的项目，以k8s add-on方式，利用k8s CRD为增加资源类型<code>VirtualMachineInstance（VMI）</code>， 使用容器的image registry去创建虚拟机并提供VM生命周期管理。 CRD的方式是的kubevirt对虚拟机的管理不用局限于pod管理接口，但是也无法使用pod的<code>RS</code> <code>DS</code> <code>Deployment</code>等管理能力，也意味着 <code>kubevirt</code>如果想要利用pod管理能力，要自主去实现，目前kubevirt实现了类似<code>RS</code>的功能。 kubevirt目前支持的runtime是docker和runv，本文中实践使用的是docker。</p>
<h3 id="kubevirt架构"><a href="#kubevirt架构" class="headerlink" title="kubevirt架构"></a>kubevirt架构</h3><p>从kubevirt架构看如何创建虚拟机，Kubevirt架构如图所示，由4部分组件组成。从架构图看出kubevirt创建虚拟机的核心就是 创建了一个特殊的pod <code>virt-launcher</code> 其中的子进程包括<code>libvirt</code>和<code>qemu</code>。做过openstack nova项目的朋友应该比较 习惯于一台宿主机中运行一个<code>libvirtd</code>后台进程，<code>kubevirt</code>中采用每个pod中一个<code>libvirt</code>进程是去中心化的模式避免因为 <code>libvirtd</code>服务异常导致所有的虚拟机无法管理。</p>
<p><img src="/2022/10/14/KubeVirt/architecture.png" alt="img"></p>
<ul>
<li><p>virt-api</p>
<p>kubevirt API服务，kubevirt是以CRD的方式工作的，virt-api提供了自定义的api请求处理，如<code>vnc</code> <code>console</code> <code>start vm</code> <code>stop vm</code>等</p>
</li>
<li><p>virt-controller</p>
<ul>
<li>与k8s api-server通讯监控<code>VMI</code>资源创建删除等状态</li>
<li>根据<code>VMI</code>定义创建<code>virt-launcher</code>pod，pod中将会运行虚拟机</li>
<li>监控pod状态，并随之更新<code>VMI</code>状态</li>
<li>监控标记为”kubevirt.io&#x2F;schedulable” node heartbeat</li>
</ul>
</li>
<li><p>virt-handler</p>
<ul>
<li>运行在kubelet的node上定期更新heartbeat，并标记”kubevirt.io&#x2F;schedulable”</li>
<li>监听在k8s apiserver当发现<code>VMI</code>被标记得nodeName与自身node匹配时，负责虚拟机的生命周期管理</li>
</ul>
</li>
<li><p>virt-launcher</p>
<ul>
<li>以pod形式运行</li>
<li>根据<code>VMI</code>定义生成虚拟机模板，通过libvirt API创建虚拟机</li>
<li>即每个虚拟机会对应对立的libvirtd</li>
<li>与libvirt通讯提供虚拟机生命周期管理</li>
</ul>
</li>
</ul>
<h3 id="虚拟机创建流程"><a href="#虚拟机创建流程" class="headerlink" title="虚拟机创建流程"></a>虚拟机创建流程</h3><ol>
<li>client 发送创建VMI命令达到k8s API server.</li>
<li>K8S API 创建VMI</li>
<li>virt-controller监听到VMI创建时，根据VMI spec生成pod spec文件，创建pods</li>
<li>k8s调度创建pods</li>
<li>virt-controller监听到pods创建后，根据pods的调度node，更新VMI 的nodeName</li>
<li>virt-handler监听到VMI nodeName与自身节点匹配后，与pod内的virt-launcher通信，virt-laucher创建虚拟机，并负责虚拟机生命周期管理</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Client                     K8s API     VMI CRD  Virt Controller         VMI Handler</span><br><span class="line">-------------------------- ----------- ------- ----------------------- ----------</span><br><span class="line"></span><br><span class="line">                           listen &lt;----------- WATCH /virtualmachines</span><br><span class="line">                           listen &lt;----------------------------------- WATCH /virtualmachines</span><br><span class="line">                                                  |                       |</span><br><span class="line">POST /virtualmachines ---&gt; validate               |                       |</span><br><span class="line">                           create ---&gt; VMI ---&gt; observe --------------&gt; observe</span><br><span class="line">                             |          |         v                       v</span><br><span class="line">                           validate &lt;--------- POST /pods              defineVMI</span><br><span class="line">                           create       |         |                       |</span><br><span class="line">                             |          |         |                       |</span><br><span class="line">                           schedPod ---------&gt; observe                    |</span><br><span class="line">                             |          |         v                       |</span><br><span class="line">                           validate &lt;--------- PUT /virtualmachines       |</span><br><span class="line">                           update ---&gt; VMI ---------------------------&gt; observe</span><br><span class="line">                             |          |         |                    launchVMI</span><br><span class="line">                             |          |         |                       |</span><br><span class="line">                             :          :         :                       :</span><br><span class="line">                             |          |         |                       |</span><br><span class="line">DELETE /virtualmachines -&gt; validate     |         |                       |</span><br><span class="line">                           delete ----&gt; * ---------------------------&gt; observe</span><br><span class="line">                             |                    |                    shutdownVMI</span><br><span class="line">                             |                    |                       |</span><br><span class="line">                             :                    :                       :</span><br></pre></td></tr></table></figure>

<h3 id="虚拟机存储"><a href="#虚拟机存储" class="headerlink" title="虚拟机存储"></a>虚拟机存储</h3><p>kubevirt目前提供了多种方式的虚拟机的磁盘</p>
<ul>
<li><p>registryDisk 可定义image来创建作为虚拟机的root disk。 virt-controller会在pod定义中创建registryVolume的container，container中的entry服务负责 将<code>spec.volumes.registryDisk.image</code>转化为qcow2格式，路径为pod根目录</p>
</li>
<li><p>cloudInitNoCloud</p>
<p>对虚拟机利用cloudinit做初始化，类似与nova中的configdrive，会根据<code>spec.volumes.cloudInitNoCloud</code> 创建包含iso文件，包含 meta-data 和 user-data。</p>
</li>
<li><p>emptyDisk</p>
<p>创建空的qcow2格式image挂载给虚拟机</p>
</li>
<li><p>PVC</p>
<p>上述几种disk都是非持久化的，随之pod的生命周期消亡，PVC是k8s提供的持久化存储。目前<code>kubevirt</code>利用pvc挂载方式都是文件系统模式挂载， PVC首先被挂载在<code>virt-laucher</code> pod中, 且需要存在名称为<code>/disk/*.img</code>的文件，才挂载给虚拟机。 file模式虚拟化方式对虚拟机磁盘存储性能有很大的影响。</p>
<p>熟悉openstack的朋友应该也了解nova-compute中如何使用ceph rbd image的，实质上是libvirt使用librbd以<code>network</code>方式 将rbd image远程改在给虚拟机。而kubevirt中将POD ip移交给了虚拟机，那将意味着pod内的libvirt服务其实是无法直接使用<code>network</code> disk的。 要么增加网络代理转发即通过host来与网络设备通讯，要么就是采用k8s volumeMount:block feature来实现。</p>
<p>kubevirt社区有<a href="https://github.com/kubevirt/kubevirt/pull/899">PR</a>已经实现了以Block的方式去使用是rbd image， 笔者手动merge并测试通过。 实质是使用了kernel rbd.ko，首先将rbd image map到host，block的mount方式 将不再以文件系统方式去挂载&#x2F;dev&#x2F;rbdx，而是为作为原始设备给pod，而pod内的libvirt就可以<code>block</code>方式将rbd image作为 磁盘挂载给虚拟机。</p>
<p>相较于PVC先格式化为文件系统并必须创建disk.img文件的使用方式，显然rbd image 以block device直接作为块设备给虚拟机少了本地文件系统层 单从存储效率讲都能提高不少。至于<code>librbd</code>和<code>rbd.ko</code>的性能本文没有对比测试，有时间再补充。</p>
<p>k8s <code>PVC</code>后续版本应该也可以提供block mode方式的mount。</p>
</li>
<li><p>dataVolume</p>
<p>dataVolume是kubevirt下的一个子项目containerized-data-importer(CDI)， 也是以CDR的方式增加<code>DataVolume</code>resource。 可以看成是从PVC和registryDisk衍生出来的，上面提过PVC使用是比较麻烦的，不仅需要PVC还需要创建disk.img， dataVolume其实将这个过程简化了，自动化的将disk.img创建在PVC中。 创建DataVolume是可以定义<code>source</code>即image&#x2F;data来源可以是<code>http</code>或者<code>s3</code>的URL，CDI controller会将 自动将image转化并拷贝到PVC文件系统<code>/disk/</code></p>
</li>
</ul>
<h3 id="虚拟机网络"><a href="#虚拟机网络" class="headerlink" title="虚拟机网络"></a>虚拟机网络</h3><p>kubevirt虚拟机网络使用的是pod网络也就是说，虚拟网络原生与pod之间是打通的。虚拟机具体的网络如图所示， virt-launcher pod网络的网卡不再挂有pod ip，而是作为虚拟机的虚拟网卡的与外部网络通信的交接物理网卡。 那么虚拟机是如何拿到pod的ip的呢，<code>virt-launcher</code>实现了简单的单ip dhcp server，就是需要虚拟机中 启动dhclient，virt-launcher服务会分配给虚拟机。</p>
<p><img src="/2022/10/14/KubeVirt/vm-networking.png" alt="img"></p>
<p>使用命令<code>kubectl exec $virt-launch-pod -c compute -- brctl show</code>可以看到bridge信息。 其中eth0就是pod与host网络通讯的veth peer网卡。也可以通过<code>virsh dumpxml</code>命令查看虚拟机的xml定义文件。</p>
<h2 id="kubevirt部署"><a href="#kubevirt部署" class="headerlink" title="kubevirt部署"></a>kubevirt部署</h2><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>环境保证网络出外网</li>
</ul>
<table>
<thead>
<tr>
<th align="left">系统</th>
<th align="left">服务</th>
</tr>
</thead>
<tbody><tr>
<td align="left">centos7</td>
<td align="left">kubernetes集群</td>
</tr>
</tbody></table>
<p>官网地址：<code>https://kubevirt.io/</code></p>
<h2 id="详细版"><a href="#详细版" class="headerlink" title="详细版"></a>详细版</h2><h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><h4 id="安装KubeVirt"><a href="#安装KubeVirt" class="headerlink" title="安装KubeVirt"></a>安装KubeVirt</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# export RELEASE=v0.35.0</span><br><span class="line">[root@master ~]# kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/$&#123;RELEASE&#125;/kubevirt-operator.yaml</span><br><span class="line">[root@master ~]# kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/$&#123;RELEASE&#125;/kubevirt-cr.yaml</span><br><span class="line">[root@master ~]# kubectl get pods -n kubevirt</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">virt-api-64999f7bf5-fblkh          1/1     Running   0          5m24s</span><br><span class="line">virt-api-64999f7bf5-l8wzr          1/1     Running   0          5m24s</span><br><span class="line">virt-controller-8696ccdf44-k9qpv   1/1     Running   0          4m52s</span><br><span class="line">virt-controller-8696ccdf44-tpxwp   1/1     Running   0          4m52s</span><br><span class="line">virt-handler-kl8tl                 1/1     Running   0          4m52s</span><br><span class="line">virt-operator-78fbcdfdf4-468h4     1/1     Running   0          8m4s</span><br><span class="line">virt-operator-78fbcdfdf4-nvvxx     1/1     Running   0          8m4s</span><br></pre></td></tr></table></figure>

<p><strong>所以服务启动后，<code>执行下面这条命令</code></strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl -n kubevirt wait kv kubevirt --for condition=Available</span><br><span class="line">kubevirt.kubevirt.io/kubevirt condition met</span><br></pre></td></tr></table></figure>

<h4 id="安装virtctl客户端工具"><a href="#安装virtctl客户端工具" class="headerlink" title="安装virtctl客户端工具"></a>安装virtctl客户端工具</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# export VERSION=v0.41.0</span><br><span class="line">[root@master ~]# wget https://github.com/kubevirt/kubevirt/releases/download/$&#123;VERSION&#125;/virtctl-$&#123;VERSION&#125;-linux-amd64</span><br><span class="line">[root@master kubevirt]# cp virtctl-v0.41.0-linux-amd64 /usr/local/bin/virtctl</span><br><span class="line">[root@master kubevirt]# chmod +x /usr/local/bin/virtctl ; ll /usr/local/bin/</span><br><span class="line">total 64000</span><br><span class="line">-r-xr-xr-x. 1 root root 17586312 Jan 11 10:52 docker-compose</span><br><span class="line">-rwxr-xr-x. 1 root root 47944250 Apr 16 16:58 virtctl</span><br><span class="line">[root@master kubevirt]# virtctl version</span><br><span class="line">Client Version: version.Info&#123;GitVersion:&quot;v0.41.0&quot;, GitCommit:&quot;b77b858ac9345ae6858cf409e1833f4548ec2809&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-11-09T13:38:36Z&quot;, GoVersion:&quot;go1.13.14&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;GitVersion:&quot;&#123;gitVersion&#125;&quot;, GitCommit:&quot;&#123;gitCommit&#125;&quot;, GitTreeState:&quot;&#123;gitTreeState&#125;&quot;, BuildDate:&quot;&#123;buildDate&#125;&quot;, GoVersion:&quot;go1.13.14&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建VirtualMachine"><a href="#创建VirtualMachine" class="headerlink" title="创建VirtualMachine"></a>创建VirtualMachine</h4><ul>
<li><code>VirtualMachine</code>(vm)类似于docker镜像一个模板可以启动很多运行实例vmi：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master kubevirt]# cat test.yaml</span><br><span class="line">apiVersion: kubevirt.io/v1alpha3</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    kubevirt.io/vm: vm-cirros</span><br><span class="line">  name: vm-cirros</span><br><span class="line">spec:</span><br><span class="line">  running: false</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        kubevirt.io/vm: vm-cirros</span><br><span class="line">    spec:</span><br><span class="line">      domain:</span><br><span class="line">        devices:</span><br><span class="line">          disks:</span><br><span class="line">          - disk:</span><br><span class="line">              bus: virtio</span><br><span class="line">            name: containerdisk</span><br><span class="line">          - disk:</span><br><span class="line">              bus: virtio</span><br><span class="line">            name: cloudinitdisk</span><br><span class="line">        machine:</span><br><span class="line">          type: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: 64M</span><br><span class="line">      terminationGracePeriodSeconds: 0</span><br><span class="line">      volumes:</span><br><span class="line">      - name: containerdisk</span><br><span class="line">        containerDisk:</span><br><span class="line">          image: kubevirt/cirros-container-disk-demo:latest</span><br><span class="line">      - cloudInitNoCloud:</span><br><span class="line">          userDataBase64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK</span><br><span class="line">        name: cloudinitdisk</span><br><span class="line">[root@master kubevirt]# kubectl apply -f test.yaml</span><br><span class="line">virtualmachine.kubevirt.io/vm-cirros created</span><br><span class="line"></span><br><span class="line">[root@master kubevirt]# kubectl get vm</span><br><span class="line">NAME        AGE   VOLUME</span><br><span class="line">vm-cirros   21m</span><br></pre></td></tr></table></figure>

<h4 id="启动VirtualMachineInstance"><a href="#启动VirtualMachineInstance" class="headerlink" title="启动VirtualMachineInstance"></a>启动VirtualMachineInstance</h4><ul>
<li><code>VirtualMachineInstance</code>（vmi）类似于docker镜像的运行实例容器：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master kubevirt]# virtctl start vm-cirros</span><br><span class="line">VM vm-cirros was scheduled to start</span><br><span class="line">[root@master kubevirt]# kubectl get vmi</span><br><span class="line">NAME        AGE   PHASE     IP            NODENAME</span><br><span class="line">vm-cirros   62s   Running   10.244.0.15   master</span><br><span class="line"></span><br><span class="line">[root@master kubevirt]# virtctl console vm-cirros  # 进入虚拟机</span><br><span class="line">Successfully connected to vm-cirros console. The escape sequence is ^]</span><br><span class="line"></span><br><span class="line">login as &#x27;cirros&#x27; user. default password: &#x27;gocubsgo&#x27;. use &#x27;sudo&#x27; for root.</span><br><span class="line">vm-cirros login: cirros</span><br><span class="line">Password:</span><br><span class="line">$ ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast qlen 1000</span><br><span class="line">    link/ether 2e:3e:2a:46:29:94 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.244.0.16/24 brd 10.244.0.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::2c3e:2aff:fe46:2994/64 scope link tentative flags 08</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">       </span><br><span class="line">$    #  按 ctrl+]  退出虚拟机</span><br><span class="line">$ [root@master kubevirt]#</span><br></pre></td></tr></table></figure>

<h4 id="启动和停止命令"><a href="#启动和停止命令" class="headerlink" title="启动和停止命令"></a>启动和停止命令</h4><p><code>spec.running</code> 字段如果设置为<code>true</code>为启动、<code>false</code>为停止**</p>
<ul>
<li>创建 <code>VirtualMachine </code>后，可以像这样打开或关闭它：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Start the virtual machine:  启动虚拟机</span><br><span class="line">virtctl start vm   </span><br><span class="line"></span><br><span class="line"># Stop the virtual machine:  停止虚拟机</span><br><span class="line">virtctl stop vm</span><br></pre></td></tr></table></figure>

<ul>
<li><code>kubectl</code>也可以使用：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Start the virtual machine:  启动虚拟机</span><br><span class="line">kubectl patch virtualmachine vm --type merge -p \</span><br><span class="line">    &#x27;&#123;&quot;spec&quot;:&#123;&quot;running&quot;:true&#125;&#125;&#x27;</span><br><span class="line"></span><br><span class="line"># Stop the virtual machine:  停止虚拟机</span><br><span class="line">kubectl patch virtualmachine vm --type merge -p \</span><br><span class="line">    &#x27;&#123;&quot;spec&quot;:&#123;&quot;running&quot;:false&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="vm作为服务公开"><a href="#vm作为服务公开" class="headerlink" title="vm作为服务公开"></a>vm作为服务公开</h4><p><code>VirtualMachine </code>可以作为服务公开。实际服务将在 <code>VirtualMachineInstance</code> 启动后可用。</p>
<p>例如， 在创建<code> VirtualMachine</code> 后，将 <code>SSH </code>端口 (22) 公开为<code>NodePort</code>服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master kubevirt]# virtctl expose virtualmachine  vm-cirros --name vmiservice-node  --target-port 22  --port 24 --type NodePort</span><br><span class="line">Service vmiservice-node successfully exposed for virtualmachine vm-cirros</span><br><span class="line">[root@master kubevirt]# kubectl get svc</span><br><span class="line">NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes        ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        95d</span><br><span class="line">vmiservice-node   NodePort    10.106.62.191   &lt;none&gt;        24:31912/TCP   3s</span><br></pre></td></tr></table></figure>

<p>最后使用<strong>远程工具连接</strong>即可：</p>
<p>注意 <strong>端口使用svc</strong> <code>NodePort端口</code>**，如图所示：</p>
<p><img src="/2022/10/14/KubeVirt/0cd0551b6f9f46bea54c1a025351faf5.png" alt="255qE5oqA5pyvTG9ncw==,size_20,color_FFFFFF,t_70,g_se,x_16)"></p>
<p><img src="/2022/10/14/KubeVirt/c962dacd9f794ad9b486efd50cfd1f78.png" alt="在这里插入图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>iptables</title>
    <url>/2022/10/20/iptables/</url>
    <content><![CDATA[<h4 id="iptables-的历史以及工作原理"><a href="#iptables-的历史以及工作原理" class="headerlink" title="iptables 的历史以及工作原理"></a>iptables 的历史以及工作原理</h4><h5 id="1-iptables的发展"><a href="#1-iptables的发展" class="headerlink" title="1.iptables的发展:"></a>1.iptables的发展:</h5><blockquote>
<p>iptables的前身叫ipfirewall （内核1.x时代）,这是一个作者从freeBSD上移植过来的，能够工作在内核当中的，对数据包进行检测的一款简易访问控制工具。但是ipfirewall工作功能极其有限(它需要将所有的规则都放进内核当中，这样规则才能够运行起来，而放进内核，这个做法一般是极其困难的)。当内核发展到2.x系列的时候，软件更名为ipchains，它可以定义多条规则，将他们串起来，共同发挥作用，而现在，它叫做iptables，可以将规则组成一个列表，实现绝对详细的访问控制功能。</p>
</blockquote>
<span id="more"></span>

<p>他们都是工作在用户空间中，定义规则的工具，本身并不算是防火墙。它们定义的规则，可以让在内核空间当中的netfilter来读取，并且实现让防火墙工作。而放入内核的地方必须要是特定的位置，必须是tcp&#x2F;ip的协议栈经过的地方。而这个tcp&#x2F;ip协议栈必须经过的地方，可以实现读取规则的地方就叫做 netfilter.(网络过滤器)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">作者一共在内核空间中选择了5个位置，</span><br><span class="line"></span><br><span class="line">1.内核空间中：从一个网络接口进来，到另一个网络接口去的</span><br><span class="line"></span><br><span class="line">2.数据包从内核流入用户空间的</span><br><span class="line"></span><br><span class="line">3.数据包从用户空间流出的</span><br><span class="line"></span><br><span class="line">4.进入/离开本机的外网接口</span><br><span class="line"></span><br><span class="line">5.进入/离开本机的内网接口</span><br></pre></td></tr></table></figure>

<p>​    </p>
<h5 id="2-iptables的工作机制"><a href="#2-iptables的工作机制" class="headerlink" title="2.iptables的工作机制"></a>2.iptables的工作机制</h5><blockquote>
<p>从上面的发展我们知道了作者选择了5个位置，来作为控制的地方，但是你有没有发现，其实前三个位置已经基本上能将路径彻底封锁了，但是为什么已经在进出的口设置了关卡之后还要在内部卡呢？ 由于数据包尚未进行路由决策，还不知道数据要走向哪里，所以在进出口是没办法实现数据过滤的。所以要在内核空间里设置转发的关卡，进入用户空间的关卡，从用户空间出去的关卡。那么，既然他们没什么用，那我们为什么还要放置他们呢？因为我们在做NAT和DNAT的时候，目标地址转换必须在路由之前转换。所以我们必须在外网而后内网的接口处进行设置关卡。     </p>
</blockquote>
<p> 这五个位置也被称为五个钩子函数（hook functions）,也叫五个规则链。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.PREROUTING (路由前)</span><br><span class="line"></span><br><span class="line">2.INPUT (数据包流入口)</span><br><span class="line"></span><br><span class="line">3.FORWARD (转发管卡)</span><br><span class="line"></span><br><span class="line">4.OUTPUT(数据包出口)</span><br><span class="line"></span><br><span class="line">5.POSTROUTING（路由后）</span><br></pre></td></tr></table></figure>

<p>​    这是NetFilter规定的五个规则链，任何一个数据包，只要经过本机，必将经过这五个链中的其中一个链。    </p>
<h5 id="3-防火墙的策略"><a href="#3-防火墙的策略" class="headerlink" title="3.防火墙的策略"></a>3.防火墙的策略</h5><p>防火墙策略一般分为两种，一种叫“通”策略，一种叫“堵”策略，通策略，默认门是关着的，必须要定义谁能进。堵策略则是，大门是洞开的，但是你必须有身份认证，否则不能进。所以我们要定义，让进来的进来，让出去的出去，所以通，是要全通，而堵，则是要选择。当我们定义的策略的时候，要分别定义多条功能，其中：定义数据包中允许或者不允许的策略，filter过滤的功能，而定义地址转换的功能的则是nat选项。为了让这些功能交替工作，我们制定出了“表”这个定义，来定义、区分各种不同的工作功能和处理方式。</p>
<p>我们现在用的比较多个功能有3个：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.filter 定义允许或者不允许的</span><br><span class="line"></span><br><span class="line">2.nat 定义地址转换的 </span><br><span class="line"></span><br><span class="line">3.mangle功能:修改报文原数据</span><br></pre></td></tr></table></figure>

<p>我们修改报文原数据就是来修改TTL的。能够实现将数据包的元数据拆开，在里面做标记&#x2F;修改内容的。而防火墙标记，其实就是靠mangle来实现的。</p>
<p>小扩展:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于filter来讲一般只能做在3个链上：INPUT ，FORWARD ，OUTPUT</span><br><span class="line"></span><br><span class="line">对于nat来讲一般也只能做在3个链上：PREROUTING ，OUTPUT ，POSTROUTING</span><br><span class="line"></span><br><span class="line">而mangle则是5个链都可以做：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING</span><br></pre></td></tr></table></figure>

<p>iptables&#x2F;netfilter（这款软件）是工作在用户空间的，它可以让规则进行生效的，本身不是一种服务，而且规则是立即生效的。而我们iptables现在被做成了一个服务，可以进行启动，停止的。启动，则将规则直接生效，停止，则将规则撤销。 </p>
<p>iptables还支持自己定义链。但是自己定义的链，必须是跟某种特定的链关联起来的。在一个关卡设定，指定当有数据的时候专门去找某个特定的链来处理，当那个链处理完之后，再返回。接着在特定的链中继续检查。</p>
<p>注意：<font color="red">规则的次序非常关键，谁的规则越严格，应该放的越靠前，而检查规则的时候，是按照从上往下的方式进行检查的。</font></p>
<h4 id="iptables命令"><a href="#iptables命令" class="headerlink" title="iptables命令"></a>iptables命令</h4><h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables(选项)(参数)</span><br></pre></td></tr></table></figure>

<h5 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-t&lt;表&gt;：指定要操纵的表；</span><br><span class="line">-A：向规则链中添加条目；</span><br><span class="line">-D：从规则链中删除条目；</span><br><span class="line">-i：向规则链中插入条目；</span><br><span class="line">-R：替换规则链中的条目；</span><br><span class="line">-L：显示规则链中已有的条目；</span><br><span class="line">-F：清楚规则链中已有的条目；</span><br><span class="line">-Z：清空规则链中的数据包计算器和字节计数器；</span><br><span class="line">-N：创建新的用户自定义规则链；</span><br><span class="line">-P：定义规则链中的默认目标；</span><br><span class="line">-h：显示帮助信息；</span><br><span class="line">-p：指定要匹配的数据包协议类型；</span><br><span class="line">-s：指定要匹配的数据包源ip地址；</span><br><span class="line">-j&lt;目标&gt;：指定要跳转的目标；</span><br><span class="line">-i&lt;网络接口&gt;：指定数据包进入本机的网络接口；</span><br><span class="line">-o&lt;网络接口&gt;：指定数据包要离开本机所使用的网络接口。</span><br></pre></td></tr></table></figure>

<p><strong>iptables命令选项输入顺序：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作</span><br></pre></td></tr></table></figure>

<p>表名包括：</p>
<ul>
<li><strong>raw</strong>：高级功能，如：网址过滤。</li>
<li><strong>mangle</strong>：数据包修改（QOS），用于实现服务质量。</li>
<li><strong>net</strong>：地址转换，用于网关路由器。</li>
<li><strong>filter</strong>：包过滤，用于防火墙规则。</li>
</ul>
<p>规则链名包括：</p>
<ul>
<li><strong>INPUT链</strong>：处理输入数据包。</li>
<li><strong>OUTPUT链</strong>：处理输出数据包。</li>
<li><strong>PORWARD链</strong>：处理转发数据包。</li>
<li><strong>PREROUTING链</strong>：用于目标地址转换（DNAT）。</li>
<li><strong>POSTOUTING链</strong>：用于源地址转换（SNAT）。</li>
</ul>
<p>动作包括：</p>
<ul>
<li>**<a href="http://lnmp.ailinux.net/accept">accept</a>**：接收数据包。</li>
<li><strong>DROP</strong>：丢弃数据包。</li>
<li><strong>REDIRECT</strong>：重定向、映射、透明代理。</li>
<li><strong>SNAT</strong>：源地址转换。</li>
<li><strong>DNAT</strong>：目标地址转换。</li>
<li><strong>MASQUERADE</strong>：IP伪装（NAT），用于ADSL。</li>
<li>**<a href="http://lnmp.ailinux.net/log">log</a>**：日志记录。</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>kubernetes笔记</title>
    <url>/2022/08/22/kubernetes%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="kubernetes三种IP"><a href="#kubernetes三种IP" class="headerlink" title="kubernetes三种IP"></a>kubernetes三种IP</h2><ul>
<li><p>Node IP：Node节点的IP地址，即物理网卡的IP地址。</p>
<blockquote>
<p>可以是物理机的IP（也可能是虚拟机IP）。每个Service都会在Node节点上开通一个端口，外部可以通过NodeIP:NodePort即可访问Service里的Pod,和我们访问服务器部署的项目一样，IP:端口&#x2F;项目名</p>
</blockquote>
</li>
<li><p>Pod IP：Pod的IP地址，即docker容器的IP地址，此为虚拟IP地址。</p>
<blockquote>
<p>Pod IP是每个Pod的IP地址，他是Docker Engine根据docker网桥的IP地址段进行分配的，通常是一个虚拟的二层网络</p>
<p>同Service下的pod可以直接根据PodIP相互通信<br>不同Service下的pod在集群间pod通信要借助于 cluster ip<br>pod和集群外通信，要借助于node ip</p>
</blockquote>
</li>
<li><p>Cluster IP：Service的IP地址，此为虚拟IP地址。</p>
<blockquote>
<p>Service的IP地址，此为虚拟IP地址。外部网络无法ping通，只有kubernetes集群内部访问使用。</p>
</blockquote>
</li>
</ul>
<h4 id="查看集群节点开放端口"><a href="#查看集群节点开放端口" class="headerlink" title="查看集群节点开放端口"></a>查看集群节点开放端口</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ss -tnl    查看k8s-node1，k8s-node2没有创建pod的时候的开放的端口</span><br></pre></td></tr></table></figure>

<h4 id="service"><a href="#service" class="headerlink" title="service"></a>service</h4><p>port<br>K8s集群内部服务访问service的入口。是service暴露在Cluster上的端口，ClusterIP:Port。如下面的yaml配置文件所示，K8s集群内部节点可以通过30080端口访问Nginx服务，然而外部网络还是不能够访问到服务，因为nodePort参数没有配置。</p>
<p>targetPort</p>
<p>容器的端口，也是最终底层的服务所提供的端口，所以说targetPod也就是Pod的端口。从port或者是nodePort进入的流量，经过路由转发之后，最终都会都通过targetPort进入到Pod中。</p>
<p><font color="red">先查看svc</font></p>
]]></content>
  </entry>
</search>
